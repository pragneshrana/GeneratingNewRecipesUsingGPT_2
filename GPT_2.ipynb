{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHr7z0H7Y4vUisSQH9mIfg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragneshrana/GeneratingNewRecipesUsingGPT_2/blob/main/GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment - 5 Retraining GPT model\n",
        "Upload run_experiments.sh, run_lm_finetuning.py and dataset_train.txt files and then run the below code.\n"
      ],
      "metadata": {
        "id": "St4iFNZhVTl8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZWmMPXXePQPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c33c9d45-c40b-4e98-aabb-e222d0d6cf0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 156931, done.\u001b[K\n",
            "remote: Counting objects: 100% (1958/1958), done.\u001b[K\n",
            "remote: Compressing objects: 100% (704/704), done.\u001b[K\n",
            "remote: Total 156931 (delta 1175), reused 1661 (delta 1046), pack-reused 154973\u001b[K\n",
            "Receiving objects: 100% (156931/156931), 158.77 MiB | 27.24 MiB/s, done.\n",
            "Resolving deltas: 100% (117351/117351), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers;\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd transformers;pip install .;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eyY4v7jQkFX",
        "outputId": "74cecaa1-4b46-468d-82a7-b61f27e7afb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers==4.32.0.dev0)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.32.0.dev0) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.32.0.dev0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/7.8 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwQrsP1dR3Pu",
        "outputId": "c8e88a2e-0ce3-4c3d-f646-5880ec92b53c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.32.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash run_experiments.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEWbnH5hR-hs",
        "outputId": "a387b9e5-91b8-45d2-f1d7-d4c9aee6baf2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘experiments’: File exists\n",
            "2023-08-20 19:03:05.381956: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/20/2023 19:03:08 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n",
            "08/20/2023 19:03:08 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=experiments/epochs_2/runs/Aug20_19-03-07_0da05903cf45,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=experiments/epochs_2,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=1,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=experiments/epochs_2,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:1468: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 50257. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1498: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "{'loss': 1.9486, 'learning_rate': 4.766049036122029e-05, 'epoch': 0.09}\n",
            "{'loss': 1.7339, 'learning_rate': 4.532098072244058e-05, 'epoch': 0.19}\n",
            "{'loss': 1.6373, 'learning_rate': 4.298147108366087e-05, 'epoch': 0.28}\n",
            "{'loss': 1.5671, 'learning_rate': 4.064196144488115e-05, 'epoch': 0.37}\n",
            "{'loss': 1.5344, 'learning_rate': 3.830245180610144e-05, 'epoch': 0.47}\n",
            "{'loss': 1.5136, 'learning_rate': 3.596294216732173e-05, 'epoch': 0.56}\n",
            "{'loss': 1.499, 'learning_rate': 3.362343252854202e-05, 'epoch': 0.66}\n",
            "{'loss': 1.4806, 'learning_rate': 3.1283922889762304e-05, 'epoch': 0.75}\n",
            "{'loss': 1.4405, 'learning_rate': 2.8944413250982594e-05, 'epoch': 0.84}\n",
            "{'loss': 1.4534, 'learning_rate': 2.6604903612202886e-05, 'epoch': 0.94}\n",
            "{'loss': 1.4253, 'learning_rate': 2.4265393973423172e-05, 'epoch': 1.03}\n",
            "{'loss': 1.411, 'learning_rate': 2.192588433464346e-05, 'epoch': 1.12}\n",
            "{'loss': 1.3834, 'learning_rate': 1.9586374695863748e-05, 'epoch': 1.22}\n",
            "{'loss': 1.3754, 'learning_rate': 1.7246865057084037e-05, 'epoch': 1.31}\n",
            "{'loss': 1.3715, 'learning_rate': 1.4907355418304325e-05, 'epoch': 1.4}\n",
            "{'loss': 1.3723, 'learning_rate': 1.2567845779524611e-05, 'epoch': 1.5}\n",
            "{'loss': 1.3504, 'learning_rate': 1.02283361407449e-05, 'epoch': 1.59}\n",
            "{'loss': 1.3664, 'learning_rate': 7.888826501965188e-06, 'epoch': 1.68}\n",
            "{'loss': 1.3519, 'learning_rate': 5.549316863185477e-06, 'epoch': 1.78}\n",
            "{'loss': 1.3582, 'learning_rate': 3.209807224405765e-06, 'epoch': 1.87}\n",
            "{'loss': 1.3469, 'learning_rate': 8.702975856260528e-07, 'epoch': 1.97}\n",
            "{'train_runtime': 2778.1661, 'train_samples_per_second': 3.846, 'train_steps_per_second': 3.846, 'train_loss': 1.4702162488265897, 'epoch': 2.0}\n",
            "100% 10686/10686 [46:18<00:00,  3.85it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/run_lm_finetuning.py\", line 296, in <module>\n",
            "    main()\n",
            "  File \"/content/run_lm_finetuning.py\", line 264, in main\n",
            "    if trainer.is_world_master():\n",
            "AttributeError: 'Trainer' object has no attribute 'is_world_master'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash run_experiments.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNYsSD2PSEiN",
        "outputId": "8bfb54e9-c5be-4a10-ac0a-3ae2b6f9b9c5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘experiments’: File exists\n",
            "2023-08-20 19:55:35.323960: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "08/20/2023 19:55:39 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n",
            "08/20/2023 19:55:39 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=experiments/epochs_2/runs/Aug20_19-55-39_0da05903cf45,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=experiments/epochs_2,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=1,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=experiments/epochs_2,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:1468: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 50257. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r experiments.zip experiments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTlYHAHFhFPk",
        "outputId": "719ea351-1acf-4a00-bf7f-e268b60b8281"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: experiments/ (stored 0%)\n",
            "  adding: experiments/epochs_2/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-4000/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-4000/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-4000/scheduler.pt (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-4000/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-4000/trainer_state.json (deflated 68%)\n",
            "  adding: experiments/epochs_2/checkpoint-4000/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-4000/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-4000/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-4000/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-5000/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-5000/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-5000/scheduler.pt (deflated 48%)\n",
            "  adding: experiments/epochs_2/checkpoint-5000/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-5000/trainer_state.json (deflated 70%)\n",
            "  adding: experiments/epochs_2/checkpoint-5000/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-5000/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-5000/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-5000/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-1000/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-1000/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-1000/scheduler.pt (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-1000/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-1000/trainer_state.json (deflated 53%)\n",
            "  adding: experiments/epochs_2/checkpoint-1000/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-1000/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-1000/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-1000/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-2000/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-2000/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-2000/scheduler.pt (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-2000/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-2000/trainer_state.json (deflated 61%)\n",
            "  adding: experiments/epochs_2/checkpoint-2000/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-2000/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-2000/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-2000/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/special_tokens_map.json (deflated 52%)\n",
            "  adding: experiments/epochs_2/runs/ (stored 0%)\n",
            "  adding: experiments/epochs_2/runs/Aug20_19-03-07_0da05903cf45/ (stored 0%)\n",
            "  adding: experiments/epochs_2/runs/Aug20_19-03-07_0da05903cf45/events.out.tfevents.1692558196.0da05903cf45.8591.0 (deflated 61%)\n",
            "  adding: experiments/epochs_2/tokenizer_config.json (deflated 41%)\n",
            "  adding: experiments/epochs_2/checkpoint-7000/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-7000/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-7000/scheduler.pt (deflated 48%)\n",
            "  adding: experiments/epochs_2/checkpoint-7000/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-7000/trainer_state.json (deflated 73%)\n",
            "  adding: experiments/epochs_2/checkpoint-7000/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-7000/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-7000/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-7000/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/tokenizer.json (deflated 72%)\n",
            "  adding: experiments/epochs_2/checkpoint-4500/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-4500/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-4500/scheduler.pt (deflated 48%)\n",
            "  adding: experiments/epochs_2/checkpoint-4500/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-4500/trainer_state.json (deflated 69%)\n",
            "  adding: experiments/epochs_2/checkpoint-4500/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-4500/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-4500/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-4500/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-2500/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-2500/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-2500/scheduler.pt (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-2500/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-2500/trainer_state.json (deflated 63%)\n",
            "  adding: experiments/epochs_2/checkpoint-2500/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-2500/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-2500/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-2500/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-10500/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-10500/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-10500/scheduler.pt (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-10500/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-10500/trainer_state.json (deflated 76%)\n",
            "  adding: experiments/epochs_2/checkpoint-10500/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-10500/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-10500/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-10500/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-5500/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-5500/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-5500/scheduler.pt (deflated 48%)\n",
            "  adding: experiments/epochs_2/checkpoint-5500/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-5500/trainer_state.json (deflated 71%)\n",
            "  adding: experiments/epochs_2/checkpoint-5500/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-5500/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-5500/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-5500/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/vocab.json (deflated 59%)\n",
            "  adding: experiments/epochs_2/merges.txt (deflated 53%)\n",
            "  adding: experiments/epochs_2/checkpoint-6000/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-6000/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-6000/scheduler.pt (deflated 48%)\n",
            "  adding: experiments/epochs_2/checkpoint-6000/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-6000/trainer_state.json (deflated 72%)\n",
            "  adding: experiments/epochs_2/checkpoint-6000/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-6000/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-6000/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-6000/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-8500/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-8500/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-8500/scheduler.pt (deflated 48%)\n",
            "  adding: experiments/epochs_2/checkpoint-8500/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-8500/trainer_state.json (deflated 74%)\n",
            "  adding: experiments/epochs_2/checkpoint-8500/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-8500/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-8500/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-8500/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-8000/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-8000/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-8000/scheduler.pt (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-8000/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-8000/trainer_state.json (deflated 74%)\n",
            "  adding: experiments/epochs_2/checkpoint-8000/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-8000/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-8000/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-8000/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-10000/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-10000/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-10000/scheduler.pt (deflated 48%)\n",
            "  adding: experiments/epochs_2/checkpoint-10000/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-10000/trainer_state.json (deflated 75%)\n",
            "  adding: experiments/epochs_2/checkpoint-10000/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-10000/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-10000/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-10000/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-1500/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-1500/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-1500/scheduler.pt (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-1500/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-1500/trainer_state.json (deflated 58%)\n",
            "  adding: experiments/epochs_2/checkpoint-1500/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-1500/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-1500/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-1500/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-3000/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-3000/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-3000/scheduler.pt (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-3000/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-3000/trainer_state.json (deflated 65%)\n",
            "  adding: experiments/epochs_2/checkpoint-3000/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-3000/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-3000/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-3000/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-3500/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-3500/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-3500/scheduler.pt (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-3500/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-3500/trainer_state.json (deflated 67%)\n",
            "  adding: experiments/epochs_2/checkpoint-3500/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-3500/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-3500/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-3500/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-6500/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-6500/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-6500/scheduler.pt (deflated 48%)\n",
            "  adding: experiments/epochs_2/checkpoint-6500/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-6500/trainer_state.json (deflated 73%)\n",
            "  adding: experiments/epochs_2/checkpoint-6500/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-6500/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-6500/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-6500/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-9000/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-9000/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-9000/scheduler.pt (deflated 48%)\n",
            "  adding: experiments/epochs_2/checkpoint-9000/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-9000/trainer_state.json (deflated 74%)\n",
            "  adding: experiments/epochs_2/checkpoint-9000/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-9000/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-9000/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-9000/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-500/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-500/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-500/scheduler.pt (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-500/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-500/trainer_state.json (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-500/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-500/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-500/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-500/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-7500/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-7500/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-7500/scheduler.pt (deflated 48%)\n",
            "  adding: experiments/epochs_2/checkpoint-7500/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-7500/trainer_state.json (deflated 74%)\n",
            "  adding: experiments/epochs_2/checkpoint-7500/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-7500/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-7500/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-7500/pytorch_model.bin (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-9500/ (stored 0%)\n",
            "  adding: experiments/epochs_2/checkpoint-9500/optimizer.pt (deflated 7%)\n",
            "  adding: experiments/epochs_2/checkpoint-9500/scheduler.pt (deflated 48%)\n",
            "  adding: experiments/epochs_2/checkpoint-9500/rng_state.pth (deflated 28%)\n",
            "  adding: experiments/epochs_2/checkpoint-9500/trainer_state.json (deflated 75%)\n",
            "  adding: experiments/epochs_2/checkpoint-9500/config.json (deflated 51%)\n",
            "  adding: experiments/epochs_2/checkpoint-9500/training_args.bin (deflated 49%)\n",
            "  adding: experiments/epochs_2/checkpoint-9500/generation_config.json (deflated 24%)\n",
            "  adding: experiments/epochs_2/checkpoint-9500/pytorch_model.bin (deflated 7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xAIu6X7_h4Qx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}